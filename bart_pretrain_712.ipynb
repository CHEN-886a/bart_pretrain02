{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1YXLKExM8zPLvXTLqbKZHOzSb9XX9I-lC",
      "authorship_tag": "ABX9TyOwL9Xfhw+W8R1eDTddQRE7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CHEN-886a/bart_pretrain02/blob/main/bart_pretrain_712.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pZIHGt4YsEG",
        "outputId": "0aa049ab-2501-4e07-e4bc-06291d3beea7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#first existing model args\n",
        "import torch\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
        "import json\n",
        "\n",
        "class FinancialNewsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, file_path, tokenizer, max_length=512):\n",
        "        self.data = []\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                self.data.append(json.loads(line))\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        record = self.data[idx]\n",
        "        company_name = record[\"input\"][\"company_name\"]\n",
        "        news_content = record[\"input\"][\"content\"]\n",
        "        combined_input = f\"{company_name}: {news_content}\"\n",
        "        summary = record[\"output\"]\n",
        "\n",
        "        inputs = self.tokenizer(combined_input, return_tensors=\"pt\", max_length=self.max_length, truncation=True, padding=\"max_length\")\n",
        "        outputs = self.tokenizer(summary, return_tensors=\"pt\", max_length=self.max_length, truncation=True, padding=\"max_length\")\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": outputs[\"input_ids\"].squeeze()\n",
        "        }\n",
        "\n",
        "# 加载分词器和数据集\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
        "train_file_path = \"/content/drive/MyDrive/colab notebook/data/train_dataset_bart_02.jsonl\"\n",
        "test_file_path = \"/content/drive/MyDrive/colab notebook/data/test_dataset_bart_02.jsonl\"\n",
        "\n",
        "train_dataset = FinancialNewsDataset(train_file_path, tokenizer)\n",
        "test_dataset = FinancialNewsDataset(test_file_path, tokenizer)\n",
        "\n",
        "# 检查 GPU 可用性\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 加载预训练模型并调整层数\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base', num_hidden_layers=12)  # 增加Decoder层数\n",
        "model.to(device)\n",
        "\n",
        "# 设置训练参数\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/colab notebook/results',\n",
        "    num_train_epochs=5,  # 增加训练轮数\n",
        "    per_device_train_batch_size=4,  # 根据GPU内存调整批量大小\n",
        "    per_device_eval_batch_size=4,\n",
        "    save_steps=1000,  # 调整保存频率\n",
        "    save_total_limit=3,\n",
        "    logging_dir='/content/drive/MyDrive/colab notebook/logs',\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,  # 调整评估频率\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\"\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "# 定义训练器\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# 开始训练\n",
        "trainer.train()\n",
        "\n",
        "# 保存模型和分词器\n",
        "model.save_pretrained('/content/drive/MyDrive/colab notebook/results/trained_model')\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/colab notebook/results/trained_model')"
      ],
      "metadata": {
        "id": "75d8Zl4mhjLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model02 第二次调参训练\n",
        "import torch\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import json\n",
        "\n",
        "# 定义数据集类\n",
        "class FinancialNewsDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, max_length=1024):\n",
        "        self.data = []\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                self.data.append(json.loads(line))\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        record = self.data[idx]\n",
        "        company_name = record[\"input\"][\"company_name\"]\n",
        "        news_content = record[\"input\"][\"content\"]\n",
        "        combined_input = f\"{company_name}: {news_content}\"\n",
        "        summary = record[\"output\"]\n",
        "\n",
        "        inputs = self.tokenizer(combined_input, return_tensors=\"pt\", max_length=self.max_length, truncation=True, padding=\"max_length\")\n",
        "        outputs = self.tokenizer(summary, return_tensors=\"pt\", max_length=self.max_length, truncation=True, padding=\"max_length\")\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": outputs[\"input_ids\"].squeeze()\n",
        "        }\n",
        "\n",
        "# 加载分词器和数据集\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
        "train_file_path = \"/content/drive/MyDrive/colab notebook_02/dataset02/train_dataset_bart_02.jsonl\"\n",
        "test_file_path = \"/content/drive/MyDrive/colab notebook_02/dataset02/test_dataset_bart_02.jsonl\"\n",
        "\n",
        "train_dataset = FinancialNewsDataset(train_file_path, tokenizer)\n",
        "test_dataset = FinancialNewsDataset(test_file_path, tokenizer)\n",
        "\n",
        "# 检查 GPU 可用性\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 加载预训练模型并调整层数\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base', num_hidden_layers=16)  # 增加Decoder层数到16层\n",
        "model.to(device)\n",
        "\n",
        "# 设置训练参数\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/colab notebook_02/result02',\n",
        "    num_train_epochs=6,                   # 训练轮数\n",
        "    per_device_train_batch_size=8,        # 每个GPU设备的训练批量大小\n",
        "    per_device_eval_batch_size=8,         # 每个GPU设备的评估批量大小\n",
        "    save_steps=500,                      # 模型保存频率（每多少个步骤保存一次）\n",
        "    save_total_limit=3,                   # 最多保存的模型数量\n",
        "    logging_dir='/content/drive/MyDrive/colab notebook_02/logs02',\n",
        "    logging_steps=100,                    # 日志记录频率（每多少个步骤记录一次）\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,                       # 评估频率（每多少个步骤评估一次）\n",
        "    load_best_model_at_end=True,          # 训练结束时加载最佳模型\n",
        "    metric_for_best_model=\"loss\",         # 选择最佳模型的评估指标（损失函数）\n",
        "    learning_rate=5e-5,                   # 学习率\n",
        "    weight_decay=0.01,                    # 权重衰减\n",
        "    gradient_accumulation_steps=1,        # 梯度累积步数（根据GPU内存调整）\n",
        "    fp16=True,                            # 混合精度训练\n",
        "    fp16_opt_level='O1'                   # 混合精度优化级别\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "# 定义训练器\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# 开始训练\n",
        "trainer.train()\n",
        "\n",
        "# 保存模型和分词器\n",
        "model.save_pretrained('/content/drive/MyDrive/colab notebook_02/result02/trained_model')\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/colab notebook_02/result02/trained_model')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FN4XKBTbVsKJ",
        "outputId": "a546d80f-87b9-4f90-a743-8cee3966ef91"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
            "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9150' max='9150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9150/9150 3:20:53, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.028900</td>\n",
              "      <td>0.024708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.026100</td>\n",
              "      <td>0.021496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.022500</td>\n",
              "      <td>0.019580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.018100</td>\n",
              "      <td>0.018832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.017100</td>\n",
              "      <td>0.017696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.016900</td>\n",
              "      <td>0.017107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.013300</td>\n",
              "      <td>0.016921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.012800</td>\n",
              "      <td>0.016282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.012200</td>\n",
              "      <td>0.015762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.009500</td>\n",
              "      <td>0.015960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.009700</td>\n",
              "      <td>0.015554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.009100</td>\n",
              "      <td>0.015369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.007200</td>\n",
              "      <td>0.015510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.007300</td>\n",
              "      <td>0.015344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.007300</td>\n",
              "      <td>0.015091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.005800</td>\n",
              "      <td>0.015444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.006000</td>\n",
              "      <td>0.015155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.005800</td>\n",
              "      <td>0.015113</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/colab notebook_02/result02/trained_model/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/colab notebook_02/result02/trained_model/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/colab notebook_02/result02/trained_model/vocab.json',\n",
              " '/content/drive/MyDrive/colab notebook_02/result02/trained_model/merges.txt',\n",
              " '/content/drive/MyDrive/colab notebook_02/result02/trained_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "source": [
        "!pip install transformers[torch] -U"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "uNk0smDljFxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install accelerate -U"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "6WU6inaWh_Ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use pretrained model to generate summary\n",
        "import torch\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "import json\n",
        "\n",
        "# 挂载Google Drive\n",
        "\n",
        "\n",
        "# 定义路径\n",
        "model_path = '/content/drive/MyDrive/colab notebook/results/trained_model'\n",
        "data_file_path = '/content/drive/MyDrive/colab notebook/data/total_dataset.jsonl'\n",
        "\n",
        "# 加载分词器和模型\n",
        "tokenizer = BartTokenizer.from_pretrained(model_path)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_path)\n",
        "\n",
        "# 检查是否有GPU可用，并将模型移动到GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "model.to(device)\n",
        "\n",
        "# 定义生成摘要的函数\n",
        "def generate_summary(company_name, news_content):\n",
        "    combined_input = f\"{company_name}: {news_content}\"\n",
        "    inputs = tokenizer(combined_input, return_tensors=\"pt\", max_length=1024, truncation=True, padding=\"max_length\")\n",
        "    inputs = inputs.to(device)\n",
        "    summary_ids = model.generate(inputs.input_ids, max_length=600, num_beams=4, length_penalty=2.0, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# 处理数据集并生成摘要\n",
        "output_summaries = []\n",
        "with open(data_file_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        record = json.loads(line)\n",
        "        company_name = record[\"input\"][\"company_name\"]\n",
        "        news_content = record[\"input\"][\"content\"]\n",
        "        summary = generate_summary(company_name, news_content)\n",
        "        output_summaries.append({\n",
        "            \"input\": {\n",
        "                \"company_name\": company_name,\n",
        "                \"content\": news_content\n",
        "            },\n",
        "            \"generated_summary\": summary\n",
        "        })\n",
        "\n",
        "# 保存生成的摘要到新的JSON文件\n",
        "output_file_path = '/content/drive/MyDrive/colab notebook/data/result_summary/bart_generated_summary02.jsonl'\n",
        "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
        "    for summary_record in output_summaries:\n",
        "        f.write(json.dumps(summary_record, ensure_ascii=False) + '\\n')\n",
        "\n",
        "print(\"Summaries generated and saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "collapsed": true,
        "id": "VjNQJl8TZAoz",
        "outputId": "5b1a1cb2-3225-4ded-b184-24a493fcc189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/colab notebook/data/total_dataset.jsonl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-2fe3fcab2d17>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# 处理数据集并生成摘要\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0moutput_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mrecord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/colab notebook/data/total_dataset.jsonl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#extract generated_summary data to a new jsonl file\n",
        "import json\n",
        "\n",
        "# 输入和输出文件路径\n",
        "input_jsonl_path = '/content/drive/MyDrive/colab notebook/data/result_summary/02/bart_generated_summary02.jsonl'\n",
        "output_json_path = '/content/drive/MyDrive/colab notebook/data/result_summary/02/bart_final_generated_summary02.jsonl'\n",
        "\n",
        "# 读取JSONL文件，并提取generated_summary到新的JSON文件\n",
        "generated_summaries = []\n",
        "\n",
        "with open(input_jsonl_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        record = json.loads(line.strip())\n",
        "        generated_summary = record['generated_summary']\n",
        "        generated_summaries.append({\n",
        "            'generated_summary': generated_summary\n",
        "        })\n",
        "\n",
        "# 将提取的数据写入新的JSON文件\n",
        "with open(output_json_path, 'w', encoding='utf-8') as fout:\n",
        "    json.dump(generated_summaries, fout, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Generated summaries extracted to {output_json_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qcWtmIkfclq",
        "outputId": "0f3c827e-16df-4427-f2af-e2d08ae3a526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated summaries extracted to /content/drive/MyDrive/colab notebook/data/result_summary/02/bart_final_generated_summary02.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#bleu\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import json\n",
        "generated_summary_filepath=\n",
        "# 加载生成的摘要和参考摘要\n",
        "with open('generated_summaries.json', 'r', encoding='utf-8') as f:\n",
        "    generated_summaries = json.load(f)\n",
        "\n",
        "with open('reference_summaries.json', 'r', encoding='utf-8') as f:\n",
        "    reference_summaries = json.load(f)\n",
        "\n",
        "# 计算每个生成摘要的BLEU分数\n",
        "bleu_scores = []\n",
        "for gen_summary, ref_summary in zip(generated_summaries, reference_summaries):\n",
        "    gen_text = gen_summary['generated_summary']\n",
        "    ref_text = ref_summary['reference_summary']\n",
        "\n",
        "    # 这里假设参考摘要可以是多个，按需调整\n",
        "    bleu = sentence_bleu([ref_text.split()], gen_text.split())\n",
        "    bleu_scores.append(bleu)\n",
        "\n",
        "    # 输出每个摘要的BLEU分数\n",
        "    print(f\"Generated Summary: {gen_text}\")\n",
        "    print(f\"Reference Summary: {ref_text}\")\n",
        "    print(f\"BLEU Score: {bleu}\")\n",
        "\n",
        "# 计算平均BLEU分数\n",
        "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "print(f\"Average BLEU Score: {average_bleu}\")\n"
      ],
      "metadata": {
        "id": "w62kQoZRixLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "# 加载模型和分词器\n",
        "model_path = '/content/drive/MyDrive/colab notebook/results/trained_model'\n",
        "tokenizer = BartTokenizer.from_pretrained(model_path)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_path)\n",
        "# Move the model to the device\n",
        "model.to(device)\n",
        "\n",
        "# 输入文本示例\n",
        "input_text = \"Mainland China and Hong Kong stocks ended lower, with a key index logging its fifth straight losing session. Investors were disappointed by a lack of policy stimulus measures amid a weak economic recovery, rising geopolitical tensions and foreign outflows.In France, a leftist alliance unexpectedly took top spot ahead of the far right in Sunday's election, a major upset that was set to prevent Marine Le Pen's National Rally from running the government.The weaker than expected showing for the far right was something of a relief for investors, though they also have concerns the left s plans could unwind many of President Emmanuel Macrons pro-market reforms\"\n",
        "\n",
        "# 处理输入并生成摘要\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device) # Move the input tensors to the device\n",
        "summary_ids = model.generate(inputs['input_ids'])\n",
        "\n",
        "# 解码摘要并打印结果\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "print(\"Generated Summary:\", summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJ720tyCZFXf",
        "outputId": "84e03c78-04a8-480b-cf8f-bff7db74f039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary: China Hong Kong stocks fall for 5th session\n"
          ]
        }
      ]
    }
  ]
}